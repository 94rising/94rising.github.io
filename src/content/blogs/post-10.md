---
title: "Sprint 2025-BY26-12: 노드 78% 줄이기"
author: "이순범"
description: "연말연초에 FinOps 최적화 작업하면서 배운 것들"
image:
  url: "../../assets/images/10.png"
  alt: "클라우드 리소스 최적화 개념도"
pubDate: 2026-01-08
tags: ["Sprint", "FinOps", "Kubernetes"]
---

12월 24일부터 1월 8일까지 진행한 스프린트. 연말연초라 여유로울 줄 알았는데 생각보다 굵직한 작업이 들어왔다.

## 한 일

크게 두 가지였다. 컨테이너 리소스 최적화랑 WAF 정책 구성.

리소스 최적화는 Azure Portal 메트릭 뽑아보는 데서 시작했다. 2주치 데이터 보니까 대부분 서비스가 CPU 10%, 메모리 15% 수준이더라. 2코어 4기가 할당받은 서비스가 실제로는 0.2코어 쓰고 있었던 거다. 이 정도면 과잉 프로비저닝이라고 밖에 할 말이 없었다.

결과적으로 노드를 14대에서 3대로 줄였다. DEV 4대→1대, STG 5대→1대, PRD 5대→1대. 78% 감축. 운영 방식도 Manual에서 Cluster Autoscaler로 바꿔서 트래픽 변동에 자동 대응되게 했다.

WAF는 15개 서비스에 화이트리스트 기반 정책을 붙였다. AGIC 앞단에 배치해서 서비스별로 접근 제어 관리할 수 있는 체계를 만들었다.

## 삽질한 부분

DEV 환경에서 프론트엔드 파드를 0.5코어 1기가로 내렸더니 파드가 안 뜨는 문제가 생겼다. 로그 보니까 Node.js 초기화 과정에서 메모리가 튀어서 OOMKilled 당한 거였다. 의존성 로딩, 환경 설정 파싱, 외부 서비스 연결 초기화하는 동안 순간적으로 메모리를 왕창 먹더라.

정상 운영 상태에서는 0.5기가면 충분한데, 시작하는 순간에 1기가를 넘겨버리니까 죽는 거다. 결국 1코어 1기가로 임시 상향했다.

## 배운 것

**리소스 스펙 정할 때 steady-state만 보면 안 된다.** 애플리케이션 라이프사이클 전체를 봐야 한다. 초기화, 배포, 스파이크 상황까지. Node.js나 Java처럼 초기 메모리 할당 큰 런타임은 steady-state + 20~30% 버퍼 두는 게 안전하다.

**보수적으로 시작하는 게 낫다.** 이번에 PRD 환경 배포하면서 기존 50% 수준(1코어 2기가)으로 시작했다. 트래픽 패턴이 달라질 수 있으니까. 운영 데이터 쌓이면 2차 최적화하면 된다.

**DEV에서 먼저 터뜨리는 게 진짜 중요하다.** FE 이슈를 DEV에서 발견해서 STG/PRD 장애를 막았다. 당연한 얘긴데 역시 환경별 순차 적용이 답이다.

## 다음에 할 것

- 백엔드 2차 최적화 (1코어 2기가 → 0.5코어 1기가 목표)
- 노드 스펙 자체도 검토해볼 예정 (D16s → D8s 가능성)
- 다른 프로젝트로 최적화 방법론 확산